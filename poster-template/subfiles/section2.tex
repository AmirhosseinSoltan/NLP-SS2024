%!TEX root = ../hbrs-poster.tex
\documentclass[hbrs-poster.tex]{subfiles}
\begin{document}
    \block{ Methodology}
    {
        In preprocessing step, we performed tokenization to convert text into tokens (words, sub-words, or characters) that the model can understand. Next steps included removing unnecessary characters, punctuation, or special symbols, normalization and lower casing, stemming and lemmatization.


        Next, we performed vectorization to encode text data into numerical format to capture the contextual features of the plot summaries and make them suitable for machine learning models. We utilized TF-IDF with ngram size (1,3) to capture more relationships between words. 

        
        \begin{tikzfigure}
            \includegraphics[width=0.22\textwidth, height=0.09\textheight]{figures/output4.png}
        \end{tikzfigure}

        
        Finally, we trained our own language model as well as distinct classifiers and compared their results. Our language model is a Deep Learning (DL) model with multiple dense layers, batch normalization, regularization, and dropout for multi-label classification. It uses an Adam optimizer, step decay learning rate schedule, and early stopping for training the model.
        \begin{tikzfigure} 
            \includegraphics[width=0.35\textwidth, height=0.15\textheight]{figures/Unbenanntes Diagramm.png}
        \end{tikzfigure}
    }
\end{document}
