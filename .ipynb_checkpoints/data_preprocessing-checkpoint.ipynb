{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercaser ( text : str):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = ['The nation of Panem? consists of a wealthy Capitol and twelve poorer districts.\\\n",
    "              As punishment for a past rebellion, each district must provide a boy and girl  \\\n",
    "              between the ages of 12 and 18 selected by lottery  for the annual Hunger Games.']\n",
    "lower_text = lowercaser(input_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special character removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_character_remover(text :str):\n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_character_removed =  special_character_remover(lower_text)\n",
    "special_character_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text : str):\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    return(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer(special_character_removed)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def stopwords_remover (text : List[str]) :\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    tokens = [word for word in text if word not in stop_words]\n",
    "    removed = [word for word in text if word in stop_words]\n",
    "    return tokens, removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_removed, removed_words = stopwords_remover(tokenized)\n",
    "\n",
    "print(stopwords_removed)\n",
    "# print(removed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming/lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def stem_text(text: List[str]) -> List[str] :\n",
    "        \n",
    "    doc = nlp(' '.join(text))\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    stemmed_tokens = [stemmer.stem(token.text) for token in doc if token.is_alpha]\n",
    "\n",
    "    return \" \".join(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_text = stem_text(stopwords_removed)\n",
    "# stemmed_text = [token for token in stemmed_text.split()]\n",
    "print(stemmed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing around with the Spellchecker class! You can ignore this!\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "print([spell.correction(word) for word in stemmed_text])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_creater (descriptions : List[List[str]]) -> List[str]:\n",
    "    corpus = []\n",
    "\n",
    "    for description in descriptions:\n",
    "        stemmed = stem_text(stopwords_remover(tokenizer(special_character_remover(lowercaser(description[0])))))\n",
    "        corpus.append(stemmed)\n",
    "\n",
    "    return corpus\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "class PreProcessor():\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm')    \n",
    "\n",
    "    def __init__(self, descriptions : List[List[str]]):\n",
    "\n",
    "        self.descriptions = descriptions\n",
    "        self.corpus = []\n",
    "\n",
    "    def lowercaser(self, text : List[str]) -> str:\n",
    "        return text[0].lower().replace(\"'\", \"\")\n",
    "    \n",
    "    def special_character_remover(self, text :str) -> str:\n",
    "\n",
    "        # Removes ulrs and numbers in the text\n",
    "        url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "        clean_text = re.sub(url_pattern, '', text)\n",
    "        clean_text = re.sub(r'[^a-zA-Z\\s]', '', clean_text)  \n",
    "        return clean_text \n",
    "        \n",
    "    def tokenizer(self , text : str) -> List[str]:\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "        return(tokens)\n",
    "\n",
    "    def stopwords_remover (self,  text : List[str]) -> List[str] :\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        tokens = [word for word in text if word not in stop_words]\n",
    "        # removed = [word for word in text if word in stop_words]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def stem_text(self, text: List[str]) -> List[str] :\n",
    "            \n",
    "        doc = nlp(' '.join(text))\n",
    "\n",
    "        stemmer = PorterStemmer()\n",
    "        \n",
    "        stemmed_tokens = [stemmer.stem(token.text) for token in doc if token.is_alpha]\n",
    "        \n",
    "        return stemmed_tokens\n",
    "    \n",
    "    def lemmatizer(self , list_of_tokens : List[str]) -> str:\n",
    "    \n",
    "        lemmatized_tokens_list = []\n",
    "        \n",
    "        for token in list_of_tokens: \n",
    "            token = WordNetLemmatizer().lemmatize(token)\n",
    "            lemmatized_tokens_list.append(token)\n",
    "            \n",
    "        return \" \".join(lemmatized_tokens_list)\n",
    "\n",
    "    \n",
    "    def corpus_creater (self) -> List[List[str]]:\n",
    "\n",
    "        for description in self.descriptions:\n",
    "            cleaned = self.lemmatizer(\n",
    "                                      self.stem_text(\\\n",
    "                                      self.stopwords_remover(\\\n",
    "                                      self.tokenizer(\\\n",
    "                                      self.special_character_remover(\\\n",
    "                                      self.lowercaser(description)\n",
    "                                      )))))\n",
    "            self.corpus.append(cleaned)\n",
    "\n",
    "        return self.corpus\n",
    "\n",
    "    def visualizer(self):\n",
    "\n",
    "        text = \" \".join(self.corpus)\n",
    "        wordcloud = WordCloud(\n",
    "            width=2048, \n",
    "            height=2048, \n",
    "            background_color='black', \n",
    "            mode='RGBA',\n",
    "            max_words=200, \n",
    "            colormap='prism', \n",
    "            contour_color='blue', \n",
    "            contour_width=0.5,                 \n",
    "        ).generate(text)\n",
    "        plt.figure(figsize=(15,15))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_text = [[\"The story begins with Hannah's brother, a young Jewish teen, as she is completing her senior year of high school. Her small neighborhood in Brooklyn is falling apart and SING! is one of the only traditions keeping the neighborhood alive. Newly arrived teacher, Miss Lombardo grew up in the neighborhood but returned to be their Sing Leader. One cold Christmas night, Miss Lombardo is leaving a neighborhood party when a young man hails her a cab, then attempts to mug her. In self defense, she bites his hand to release his grip and he screams in pain and terror, quickly making an exit. The cab driver jokes about not starting the meter yet. On the first day of school, Miss Lombardo runs into difficulty when her students are uninterested and misbehaved. One such student was named Dominic who gets scolded for bringing stolen watches to school grounds and putting his feet up on the desk. On the day of Sing Leader elections, Miss Lombardo recognizes Dominic as her mugger by the bandage on his hand and decides to blackmail him into being co-Sing Leader of the Senior class along with Hannah, who was rightfully elected. The school kids work hard to plan their Sing productions. Hannah and Dominic clash along the way as Hannah uses traditional Sing planning strategies while Dominic wants to introduce the flavor of the youth in the neighborhood. In order to put Dominic and Hannah on the same page, Miss Lombardo suggests that Hannah accompany Dominic to a local club. At first, the two are equally hesitant but Hannah agrees on the terms that it is not a date. However, by the end of the night, Hannah uses Dominic to make her ex boyfriend, Mickey, jealous, and due to this, Hannah and Dominic start seeing each other in a different light. Dominic accompanies Hannah on her walk home and the two share a romantic kiss. Once the two are finally uniting and getting along, the Dept. of Education informs the school that it will close its doors forever at the end of this semester and therefore, there will not be enough resources for them to complete this year's Sing. This fuels the kids to work even harder on their productions and the neighborhood comes together even more to help finance the show, despite the school authorities' ban. Ironically, just as things are starting to look up, Dominic reluctantly accompanies his brother on a robbery of Hannah's mother's diner, their sole source of income which already was at risk of failure due to the school's upcoming closure. A classmate saw Dominic standing outside the diner at the time of the crime and informed Hannah of what he saw. Devastated, Hannah confronts Dominic and he promises to get the money back for her. He then steals the money back from his brother and returns it to the diner, restoring Hannah's faith in him. The recent events had discouraged Dominic from fulfilling his co-Sing leader duties and he had been skipping out on rehearsals. In a moment of great need as the senior's main performer falls unconscious, Dominic steps in to save the show. He sheds his bad-boy demeanor and exceeds all expectations. The underclassmen and seniors perform to a record-high sold out audience. At the end of the show, Hannah makes a moving speech motivating the community to rejoice and always remember that despite compromising circumstances, they completed a successful Sing and proved their community's worth.\"],\\\n",
    "              [\"Set in a lonely city on a rainy night, the film takes place in a bicycle shop  that is closed for the night. In the corner of the shop sleeps Red, a red unicycle who languishes in the clearance corner, waiting to be purchased. As the camera zooms on him, the sound of rain falling turns into a drumroll, and we go into the dream-sequence. In his dream, Red is being ridden by a circus clown  as part of a juggling act. The clown enters the ring, accompanied by a fanfare, expecting a huge applause, but instead receives only a few scattered claps from different parts of the  audience. Nevertheless, Lumpy starts juggling three balls whilst riding Red, occasionally dropping them as he does. However, Red slides out from underneath Lumpy  and spikes the balls back to him with his bike pedals. The confused clown ponders this for only a second before continuing on with his act. At this point, Red is forced to catch another ball which Lumpy unintentionally throws across the ring. Lumpy continues to ride in the air while juggling the other two balls while Red bounces the green ball up and down. Eventually Lumpy comes to a sudden realization, and looks between his legs, only to discover he's been riding on nothing before he falls to the ground . Red catches the other two balls and begins juggling all three of them, and then balances them on top of each other, after which he receives an uproarous applause. But then the sound of clapping turns into the sound of rain, and Red awakens, left to face bleak reality. Depressed, he returns to the corner where he was previously resting, and goes back to sleep. The short ends with the final image of the neon sign for Eben's Bikes.\"],\n",
    "              [\"The president is on his way to give a speech. While he is traveling there a man shows up with a camera. A reporter tries to ask a member of the secret service a question. When the president enters he is shot by the man with the camera. The president's main bodyguard, Alex Thomas , is grazed by the bullet that hits the president. The shooter is gunned down by Alex and other secret service agents. The president dies at the hospital. Kate Crawford , an investigative journalist, starts asking questions about the assassination. Anyone she questions is killed. She goes to Alex Thomas's house to tell him what is happening. As they head to his boat, Thomas sees some men hiding in the bushes. He throws Kate into the water and dives in. Thomas jumps out of the water to kill two of the hitmen while a third hitman drives off to inform his boss what happened. They are able to link the hitmen to a man called Jack Baldwin . Agent Thomas and other Secret Service members attack the location of Jack Baldwin. Baldwin escapes but is later caught by Thomas. Thomas and Crawford are suspicious of Vaughan Stevens , Agent Thomas's boss, who had previous links to Baldwin. While reviewing film of the assassination at his house, Thomas discovers that Stevens handed the assassin a gun during the president's entrance. Thomas leaves to find Stevens while Kate stays at his house. When Thomas arrives at Stevens' home he finds him dead. Before entering, Thomas sees a car leaving the house. He then receives a call that Baldwin has escaped. Kate is attacked by Baldwin but Thomas arrives and kills Baldwin. Thomas arrives at the first lady's  home to see the car that left Steven's house pulling away. Thomas discovers that the first lady wanted her husband killed - due to the fact that the President was being unfaithful to her. A few weeks later Kate and Thomas have dinner. Thomas says that he still doesn't know who wanted the President dead.\"],              \n",
    "              ['A wife s husband is cheating on her. She decides to go on a road trip with her husband s other woman. While driving the two women pick up a hitchhiker. The man they pick up may be a robber and murderer on the run from the cops. A policeman who is tracking the hitchhiker has a close eye on them, but the question is why?http://www.imdb.com/title/tt0112515']\n",
    "              ]\n",
    "\n",
    "pp = PreProcessor(input_text)\n",
    "corrpus = pp.corpus_creater()\n",
    "corrpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "genres = [\n",
    "#  ['Drama'],\n",
    " ['Drama', 'Horror', 'Thriller'],\n",
    " ['Drama'],\n",
    " ['Children', 'Drama'],\n",
    " ['Comedy', 'Drama', 'Horror', 'Thriller'],\n",
    " ['Comedy', 'Drama','Children', 'Horror', 'Thriller']]\n",
    "\n",
    "multilabel_binarizer.fit(genres)\n",
    "\n",
    "# transform target variable\n",
    "y = multilabel_binarizer.transform(genres)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_binarizer.inverse_transform(y[[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert model for creating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# # Example data\n",
    "# text = \"Set in a lonely city on a rainy night, the film takes place in a bicycle shop  that is closed for the night. In the corner of the shop sleeps Red, a red unicycle who languishes in the clearance corner, waiting to be purchased. As the camera zooms on him, the sound of rain falling turns into a drumroll, and we go into the dream-sequence. In his dream, Red is being ridden by a circus clown  as part of a juggling act. The clown enters the ring, accompanied by a fanfare, expecting a huge applause, but instead receives only a few scattered claps from different parts of the  audience. Nevertheless, Lumpy starts juggling three balls whilst riding Red, occasionally dropping them as he does. However, Red slides out from underneath Lumpy  and spikes the balls back to him with his bike pedals. The confused clown ponders this for only a second before continuing on with his act. At this point, Red is forced to catch another ball which Lumpy unintentionally throws across the ring. Lumpy continues to ride in the air while juggling the other two balls while Red bounces the green ball up and down. Eventually Lumpy comes to a sudden realization, and looks between his legs, only to discover he's been riding on nothing before he falls to the ground . Red catches the other two balls and begins juggling all three of them, and then balances them on top of each other, after which he receives an uproarous applause. But then the sound of clapping turns into the sound of rain, and Red awakens, left to face bleak reality. Depressed, he returns to the corner where he was previously resting, and goes back to sleep. The short ends with the final image of the neon sign for Eben's Bikes.\"\n",
    "\n",
    "# # Load BERT tokenizer and model\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Tokenize input text and get BERT embeddings\n",
    "# inputs = tokenizer(text, return_tensors='pt')\n",
    "# outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = outputs.last_hidden_state.detach().numpy()\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('./Dataset_MovieSummaries/Dataset.csv')\n",
    "df = df.drop(columns=['Unnamed: 0','movie_id','movie_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the movie descriptions, creating corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "summaries = [[summary] for summary in df['summary']]\n",
    "processor = PreProcessor(summaries)\n",
    "cleaned_summaries = processor.corpus_creater()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.visualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing with processed summary\n",
    "df['summary'] = cleaned_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['summary']\n",
    "Y = df.loc[:, df.columns != 'summary']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state = 123)\n",
    "tfidf = TfidfVectorizer(max_features=10000, min_df=20 , ngram_range=(1,3))\n",
    "\n",
    "# Fit the vectorizer to the training data\n",
    "tfidf.fit(X_train)\n",
    "\n",
    "# Transform both train and test sets\n",
    "X_train_tfidf = tfidf.transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# pickle tfidf model for later use\n",
    "joblib.dump(tfidf, 'models/my_tfidf_min20.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train_tfidf_df = pd.DataFrame(X_train_tfidf.toarray())\n",
    "X_test_tfidf_df = pd.DataFrame(X_test_tfidf.toarray())\n",
    "y_train_df = pd.DataFrame(y_train)\n",
    "y_test_df = pd.DataFrame(y_test)\n",
    "\n",
    "X_train_tfidf_df.to_csv('./Dataset_MovieSummaries/X_train.csv')\n",
    "X_test_tfidf_df.to_csv('./Dataset_MovieSummaries/X_test.csv')\n",
    "y_train_df.to_csv('./Dataset_MovieSummaries/y_train.csv', header=True)\n",
    "y_test_df.to_csv('./Dataset_MovieSummaries/y_test.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
